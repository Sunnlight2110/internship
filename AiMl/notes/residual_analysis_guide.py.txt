
| Model Type                        | Homoscedasticity              | Normality of Residuals        | Autocorrelation                   | Influential Points                | Outliers              | Evaluation Metrics                                                    | Multicollinearity         |
|-----------------------------------|-------------------------------|-------------------------------|-----------------------------------|-----------------------------------|-----------------------|-----------------------------------------------------------------------|---------------------------|
| Linear Regression                 | Residuals vs Fitted, BP Test  | Q-Q Plot, Shapiro-Wilk Test   | Durbin-Watson                     | Cook's Distance, Leverage         | Studentized Residuals | R-squared, MSE,Adjusted R-squared, MAE                                | VIF, Correlation Matrix   |
| Logistic Regression               | Not Needed                    | Q-Q Plot, Shapiro-Wilk Test   | Durbin-Watson, Breusch-Godfrey    | Cook's Distance                   | Studentized Residuals | Accuracy, AUC, Confusion Matrix, Precision, Recall, F1-Score, Log-Loss| VIF, Correlation Matrix   |
| Decision Trees                    | Not Needed                    | Not Needed                    | Not Needed                        | Leverage, Feature Importance      | Feature Importance    | Accuracy, Gini, Cross-validation,entropy                              | Not Needed                |
| Random Forest                     | Not Needed                    | Not Needed                    | Not Needed                        | Feature Importance, Out-of-bag    | Feature Importance    | Accuracy, AUC, OOB Error                                              | Not Needed                |
| Support Vector Machines (SVM)     | Not Needed                    | Not Needed                    | Not Needed                        | Support Vectors                   | Decision Boundaries   | Accuracy, Cross-validation                                            | Not Needed                |
| Neural Networks (ANN, CNN, RNN)   | Not Needed                    | Not Needed                    | Not Needed                        | Activation Maps                   | Loss Function         | Accuracy, Cross-validation                                            | Not Needed                |
| K-Nearest Neighbors (KNN)         | Not Needed                    | Not Needed                    | Not Needed                        | Distance Metrics                  | Distance Measures     | Accuracy, Cross-validation                                            | Not Needed                |
| Naive Bayes                       | Not Needed                    | Not Needed                    | Not Needed                        | Not Needed                        | Not Needed            | Accuracy, Cross-validation                                            | Not Needed                |
| Poisson Regression                | Residuals vs Fitted           | Q-Q Plot                      | Durbin-Watson                     | Cook's Distance                   | Studentized Residuals | Deviance, AIC                                                         | Not Needed                |
| Multivariate Logistic Regression  | Not Needed                    | Q-Q Plot, Shapiro-Wilk Test   | Durbin-Watson, Breusch-Godfrey    | Cook's Distance                   | Studentized Residuals | Accuracy, AUC, Confusion Matrix                                       | VIF, Correlation Matrix   |
| Ridge/Lasso Regression            | Residuals vs Fitted           | Q-Q Plot, Shapiro-Wilk Test   | Durbin-Watson                     | Cook's Distance                   | Studentized Residuals | MSE, AIC, Cross-validation                                            | VIF, Correlation Matrix   |




Residuals vs Fitted Plot (Homoscedasticity)
    Purpose: To check if the residuals exhibit constant variance (homoscedasticity). Ideally, residuals should be randomly scattered around zero, with no discernible pattern.
    Interpretation: A random scatter suggests homoscedasticity; a pattern (e.g., funnel shape) suggests heteroscedasticity.
    What to Look For:
        Good: Residuals spread evenly, no shape.
        Bad: Funnel shape or pattern suggests heteroscedasticity.
    Code:
        plt.scatter(model.fittedvalues, model.resid)
        plt.axhline(0, color='red', linestyle='--')
        plt.title('Residuals vs Fitted')
        plt.xlabel('Fitted Values')
        plt.ylabel('Residuals')
        plt.show()

Breusch-Pagan Test (Homoscedasticity)
    Purpose: Formal test for heteroscedasticity. It checks if the variance of residuals is constant.
    Interpretation: A p-value less than 0.05 indicates heteroscedasticity, suggesting the residuals have non-constant variance.
    What to Look For:
        Good: p-value > 0.05 (no evidence of heteroscedasticity).
        Bad: p-value < 0.05 (suggesting heteroscedasticity).
    Code:
        from statsmodels.stats.diagnostic import het_breuschpagan
        bp_test = het_breuschpagan(model.resid, model.model.exog)
        print(f"Breusch-Pagan Test p-value: {bp_test[1]}")

Q-Q Plot (Normality of Residuals)
    Purpose: To check if residuals are normally distributed by comparing them to a theoretical normal distribution.
    Interpretation: If the points lie approximately along a straight line, the residuals are normally distributed. Deviations indicate non-normality.
    What to Look For:
        Good: Points fall along the diagonal line (normal distribution).
        Bad: Points deviate from the line (non-normal residuals).
    Code:
        import statsmodels.api as sm
        sm.qqplot(model.resid, line='45')
        plt.title('Q-Q Plot')
        plt.show()

Shapiro-Wilk Test (Normality of Residuals)
    Purpose: A formal statistical test for normality. The null hypothesis is that the residuals follow a normal distribution.
    Interpretation: A p-value less than 0.05 indicates that the residuals do not follow a normal distribution.
    What to Look For:
        Good: p-value > 0.05 (residuals are normally distributed).
        Bad: p-value < 0.05 (residuals are not normally distributed).
    Code:
        from scipy.stats import shapiro
        stat, p_value = shapiro(model.resid)
        print(f"Shapiro-Wilk Test Stat: {stat}, p-value: {p_value}")

Anderson-Darling Test (Normality of Residuals)
    Purpose: Another statistical test for normality of residuals.
    Interpretation: A p-value less than 0.05 indicates that the residuals deviate from normality.
    What to Look For:
        Good: p-value > 0.05 (residuals follow normal distribution).
        Bad: p-value < 0.05 (residuals deviate from normality).
    Code:
        from scipy.stats import anderson
        result = anderson(model.resid)
        print(f"Anderson-Darling Test Statistic: {result.statistic}, p-value: {result.significance_level}")

Kolmogorov-Smirnov Test (Normality of Residuals)
    Purpose: Compares the empirical distribution of residuals with a normal distribution.
    Interpretation: A p-value less than 0.05 suggests that the residuals do not follow a normal distribution.
    What to Look For:
        Good: p-value > 0.05 (residuals follow a normal distribution).
        Bad: p-value < 0.05 (residuals deviate from normality).
    Code:
        from scipy.stats import kstest
        stat, p_value = kstest(model.resid, 'norm')
        print(f"Kolmogorov-Smirnov Test Stat: {stat}, p-value: {p_value}")

Durbin-Watson Test (Autocorrelation of Residuals)
    Purpose: Tests for autocorrelation in residuals, which is especially useful for time series data.
    Interpretation: A value close to 2 suggests no autocorrelation. Values close to 0 or 4 indicate positive or negative autocorrelation, respectively.
    What to Look For:
        Good: Value near 2 (indicating no autocorrelation).
        Bad: Value near 0 or 4 (indicating positive or negative autocorrelation).
    Code:
        from statsmodels.stats.stattools import durbin_watson
        dw_stat = durbin_watson(model.resid)
        print(f"Durbin-Watson Statistic: {dw_stat}")

Cook's Distance (Influential Points)
    Purpose: Identifies influential data points that disproportionately affect the model’s estimates.
    Interpretation: Data points with high Cook's distance may be outliers or have a significant influence on the regression model.
    What to Look For:
        Good: Points with Cook’s distance < 1 (not influential).
        Bad: Points with Cook’s distance > 1 (potential influential points).
    Code:
        influence = model.get_influence()
        cooks_d = influence.cooks_distance[0]
        plt.stem(np.arange(len(cooks_d)), cooks_d, markerfmt="o", basefmt=" ", use_line_collection=True)
        plt.title('Cook\'s Distance for Each Data Point')
        plt.xlabel('Index')
        plt.ylabel('Cook\'s Distance')
        plt.show()

Leverage (Influential Points)
    Purpose: Measures how much a data point influences the regression line. High leverage points have extreme values for the predictor variables.
    Interpretation: Data points with high leverage are more likely to have a large influence on the regression model and should be carefully reviewed.
    What to Look For:
        Good: Points with leverage < 2*(p/n) (no influence).
        Bad: Points with leverage > 2*(p/n) (high influence, review carefully).
    Code:
        leverage = influence.hat_matrix_diag
        plt.stem(np.arange(len(leverage)), leverage, markerfmt="o", basefmt=" ", use_line_collection=True)
        plt.title('Leverage for Each Data Point')
        plt.xlabel('Index')
        plt.ylabel('Leverage')
        plt.show()

R-squared
    Purpose: Measures the proportion of variance in the dependent variable that is predictable from the independent variables. It indicates how well the independent variables explain the variation in the dependent variable.
    Interpretation: A higher R-squared means a better fit of the model to the data.
    What to Look For:
        Good: Higher R-squared (close to 1) indicates a good model fit.
        Bad: Lower R-squared (close to 0) suggests that the model doesn’t explain much of the variance.
    Code:
        from sklearn.metrics import r2_score
        r2 = r2_score(y_true, y_pred)
        print(f"R-squared: {r2}")

MSE (Mean Squared Error)
    Purpose: Measures the average squared difference between predicted and actual values. It’s a commonly used metric for regression models, with a lower value indicating better performance.
    Interpretation: Lower MSE means better model performance.
    What to Look For:
        Good: Lower MSE values indicate better accuracy.
        Bad: Higher MSE values suggest poor model performance.
    Code:
        from sklearn.metrics import mean_squared_error
        mse = mean_squared_error(y_true, y_pred)
        print(f"MSE: {mse}")

Adjusted R-squared
    Purpose: Measures the proportion of variance in the dependent variable explained by the independent variables, adjusting for the number of predictors. It’s used to compare models with different numbers of predictors.
    Interpretation: A higher Adjusted R-squared indicates a better fit of the model, but it should be used with caution.
    What to Look For:
        Good: Higher values indicate better explanatory power, but beware of overfitting.
        Bad: A low or negative value suggests that the model is poorly fitted or may be overfitting.
    Code:
        from sklearn.metrics import r2_score
        adjusted_r2 = 1 - (1 - model.score(X, y)) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
        print(f"Adjusted R-squared: {adjusted_r2}")

MAE (Mean Absolute Error)
    Purpose: Measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation.
    Interpretation: A lower MAE indicates better model accuracy.
    What to Look For:
        Good: Lower MAE indicates better prediction accuracy.
        Bad: Higher MAE indicates that the model’s predictions are far from actual values.
    Code:
        from sklearn.metrics import mean_absolute_error
        mae = mean_absolute_error(y_true, y_pred)
        print(f"MAE: {mae}")

Studentized Residuals (Outliers)
    Purpose: Identifies outliers by measuring the difference between the observed value and the predicted value, adjusted for the variance of the residuals.
    Interpretation: Large studentized residuals (greater than ±3) indicate outliers.
    What to Look For:
        Good: Residuals between -3 and +3 (no outliers).
        Bad: Residuals greater than 3 or less than -3 (indicating outliers).
    Code:
        from statsmodels.stats.outliers_influence import OLSInfluence
        influence = OLSInfluence(model)
        studentized_residuals = influence.resid_studentized_internal
        plt.stem(np.arange(len(studentized_residuals)), studentized_residuals, markerfmt="o", basefmt=" ", use_line_collection=True)
        plt.title('Studentized Residuals for Each Data Point')
        plt.xlabel('Index')
        plt.ylabel('Studentized Residuals')
        plt.show()

Accuracy, AUC, Confusion Matrix, Precision, Recall, F1-Score, Log-Loss
    Purpose: To assess the performance of a Logistic Regression model in terms of both classification accuracy and model discrimination ability. These metrics also measure how well the model is classifying different classes, handling imbalances, and managing prediction uncertainty.
    Interpretation:
        Accuracy: Overall percentage of correct predictions.
        AUC: Measures the model's ability to distinguish between classes (higher is better).
        Confusion Matrix: A detailed breakdown of correct and incorrect predictions (True Positives, False Positives, True Negatives, False Negatives).
        Precision: The proportion of true positives among all predicted positives (high precision means few false positives).
        Recall: The proportion of true positives among all actual positives (high recall means fewer false negatives).
        F1-Score: The harmonic mean of Precision and Recall, used when you need a balance between Precision and Recall.
        Log-Loss: The penalty for incorrect classifications, where lower values indicate better performance.
    What to Look For:
        Good:
        Accuracy > 90% (depending on context).
        AUC close to 1.
        Confusion Matrix with high true positive and true negative values.
        Precision, Recall, F1-Score close to 1.
        Log-Loss close to 0.
        Bad:
        Low Accuracy, AUC, and unbalanced Confusion Matrix.
        Precision, Recall, F1-Score far from 1.
        High Log-Loss.
    Code:
        from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, log_loss

        # Predicting
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]  # For AUC and Log-Loss

        # Accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # AUC
        auc = roc_auc_score(y_test, y_prob)

        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)

        # Precision, Recall, F1-Score
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Log-Loss
        logloss = log_loss(y_test, y_prob)

        print(f"Accuracy: {accuracy}")
        print(f"AUC: {auc}")
        print(f"Confusion Matrix:\n{cm}")
        print(f"Precision: {precision}")
        print(f"Recall: {recall}")
        print(f"F1-Score: {f1}")
        print(f"Log-Loss: {logloss}")

Breusch-Godfrey Test (Autocorrelation)
    Purpose: Detects autocorrelation at higher lags, useful when time-series data has lagged dependencies.
    Interpretation: A significant p-value suggests that residuals are autocorrelated at one or more lags.
    What to Look For:
        Good: p-value > 0.05 (no autocorrelation in residuals).
        Bad: p-value < 0.05 (evidence of autocorrelation).
    Code:
        from statsmodels.stats.diagnostic import acorr_breusch_godfrey
        bg_test = acorr_breusch_godfrey(model)
        print(f"Breusch-Godfrey Test p-value: {bg_test[1]}")

Variance Inflation Factor (VIF)
    Purpose: Detects multicollinearity by measuring how much variance of a feature is inflated due to correlation with other features.
    Interpretation:
        VIF > 10 → High multicollinearity (problematic)
        VIF between 5-10 → Moderate multicollinearity (watch out)
        VIF < 5 → Low multicollinearity (good)
    What to Look For:
        Good: VIF < 5 (low multicollinearity).
        Bad: VIF > 5 or 10 (high multicollinearity).
    Code:
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        import pandas as pd

        X = df[['feature1', 'feature2', 'feature3']]  # Select independent variables
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X.columns
        vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        print(vif_data)

Correlation Matrix
Purpose: Shows how strongly features are related to each other, helping to detect multicollinearity.
    Interpretation:
        Correlation close to +1 or -1 → High collinearity (problematic)
        Correlation close to 0 → No multicollinearity (good)
    What to Look For:
        Good: Correlation < 0.8 or 0.9 (no multicollinearity).
        Bad: Correlation > 0.8 or 0.9 (multicollinearity).
    Code:
        import seaborn as sns
        import matplotlib.pyplot as plt

        corr_matrix = df.corr()
        plt.figure(figsize=(8, 6))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
        plt.title('Correlation Matrix')
        plt.show()

Accuracy
    Purpose: Measures the proportion of correct predictions made by the model, indicating overall performance.
    Interpretation:
        - High accuracy indicates a better model.
        - Low accuracy may suggest a poor model or underfitting.
    What to Look For:
        - Good: High accuracy (close to 1).
        - Bad: Low accuracy (close to 0).
    Code:
        from sklearn.metrics import accuracy_score  
        accuracy = accuracy_score(y_true, y_pred)  
        print(f"Accuracy: {accuracy}") 


AUC (Area Under the Curve)
    Purpose: Measures the area under the ROC curve, helping to evaluate the ability of the model to discriminate between positive and negative classes.
    Interpretation:
        - AUC closer to 1 means better discrimination ability.
        - AUC closer to 0.5 suggests the model is no better than random guessing.
    What to Look For:
        - Good: AUC > 0.8
        - Bad: AUC < 0.5
    Code:
        from sklearn.metrics import roc_auc_score  
        auc = roc_auc_score(y_true, y_pred_prob)  
        print(f"AUC: {auc}")

Confusion Matrix
    Purpose: A matrix showing the performance of the classification model by comparing actual versus predicted labels. It helps identify true positives, true negatives, false positives, and false negatives.
    Interpretation:
        - Diagonal values (true positives and true negatives) represent correct classifications.
        - Off-diagonal values (false positives and false negatives) represent errors.
    What to Look For:
        - Good: High diagonal values (accurate classifications).
        - Bad: High off-diagonal values (misclassifications).
    Code:
        from sklearn.metrics import confusion_matrix  
        cm = confusion_matrix(y_true, y_pred)  
        print("Confusion Matrix:")  
        print(cm)

Feature Importance
    Purpose: Determines which features contribute the most to the model's predictions, helping in feature selection.
    Interpretation: Higher values indicate more important features, while lower values suggest less important ones.
    What to Look For:
        Good: Features with higher importance contribute meaningfully to the model's predictions.
        Bad: Features with low importance might be removed to simplify the model without losing predictive power.
    Code:
        from sklearn.ensemble import RandomForestClassifier
        import matplotlib.pyplot as plt

        # Train the model
        model = RandomForestClassifier()
        model.fit(X_train, y_train)

        # Plot Feature Importance
        importances = model.feature_importances_
        indices = np.argsort(importances)

        plt.figure(figsize=(10, 6))
        plt.title('Feature Importance')
        plt.barh(range(X_train.shape[1]), importances[indices], align="center")
        plt.yticks(range(X_train.shape[1]), X_train.columns[indices])
        plt.xlabel('Relative Importance')
        plt.show()

Gini Impurity
    Purpose: Measures the "impurity" of a dataset or the node in decision tree algorithms. It helps in understanding how the data is split based on different features.
    Interpretation: The Gini index ranges from 0 (perfectly pure) to 1 (maximally impure). Lower values indicate a better split (purity).
    What to Look For:
        Good: Lower Gini values after splitting, indicating pure groups.
        Bad: Higher Gini values suggesting mixed groups (less optimal splits).
    Code:
        from sklearn.tree import DecisionTreeClassifier

        # Train the model
        model = DecisionTreeClassifier(criterion='gini')
        model.fit(X_train, y_train)

        # Access the Gini Impurity
        print("Gini Impurity of the model:", model.score(X_test, y_test))

Entropy (for Decision Trees)
    Purpose: To measure the disorder or impurity of a dataset. Lower entropy means more homogeneous nodes, while higher entropy means more mixed nodes. It's the basis for Information Gain used in Decision Trees.

    Interpretation: The goal is to minimize entropy during the tree's construction. The node with the lowest entropy is chosen to split the data.
        Good: Lower entropy, more uniform distribution of classes.
        Bad: Higher entropy, more disorder in the dataset.
    Code:

        from sklearn.tree import DecisionTreeClassifier
        from sklearn.datasets import load_iris

        # Load sample data
        data = load_iris()
        X = data.data
        y = data.target

        # Create Decision Tree with entropy
        tree = DecisionTreeClassifier(criterion='entropy')
        tree.fit(X, y)

Cross-Validation
    Purpose: Evaluates the model's performance by splitting the dataset into multiple parts (folds) and training/testing the model on each part. It helps ensure the model generalizes well and is not overfitting.
    Interpretation: Cross-validation gives you a more reliable estimate of the model's performance compared to a single train-test split.
    What to Look For:
        Good: High and consistent performance across different folds (e.g., similar accuracy across all splits).
        Bad: Large variance in performance (e.g., high accuracy in some folds and low in others) may indicate overfitting.
    Code:
        from sklearn.model_selection import cross_val_score
        from sklearn.ensemble import RandomForestClassifier

        # Initialize model
        model = RandomForestClassifier()

        # Perform cross-validation
        cv_scores = cross_val_score(model, X, y, cv=5)

        print("Cross-Validation Scores:", cv_scores)
        print("Average Cross-Validation Score:", cv_scores.mean())