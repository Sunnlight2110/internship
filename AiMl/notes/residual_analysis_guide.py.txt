
| Model Type                       | Homoscedasticity               | Normality of Residuals        | Autocorrelation                   | Influential Points            | Outliers              | Evaluation Metrics            |
|----------------------------------|--------------------------------|-------------------------------|-----------------------------------|-------------------------------|-----------------------|-------------------------------|
| Linear Regression                | Residuals vs Fitted, BP Test   | Q-Q Plot, Shapiro-Wilk Test   | Durbin-Watson                     | Cook's Distance, Leverage     | Studentized Residuals | R-squared, MSE                |
| Time Series (ARIMA, SARIMA)      | Residuals vs Fitted            | Q-Q Plot                      | Durbin-Watson, Breusch-Godfrey    | Cook's Distance               | Studentized Residuals | AIC, BIC                      |
| Decision Trees, Random Forests   | Not needed                     | Not needed                    | Not needed                        | Leverage, Feature Importance  | Feature Importance    | R-squared, Cross-validation   |
| Support Vector Machines (SVM)    | Not needed                     | Not needed                    | Not needed                        | Support Vectors               | Decision Boundaries   | Accuracy, Cross-validation    |
| Neural Networks (ANN, CNN, RNN)  | Not needed                     | Not needed                    | Not needed                        | Activation Maps               | Loss Function         | Accuracy, Cross-validation    |
| K-Nearest Neighbors (KNN)        | Not needed                     | Not needed                    | Not needed                        | Distance Metrics              | Distance Measures     | Accuracy, Cross-validation    |





Residuals vs Fitted Plot (Homoscedasticity)
Purpose: To check if the residuals exhibit constant variance (homoscedasticity). Ideally, residuals should be randomly scattered around zero, with no discernible pattern.
Interpretation: A random scatter suggests homoscedasticity; a pattern (e.g., funnel shape) suggests heteroscedasticity.
Code:
    plt.scatter(model.fittedvalues, model.resid)
    plt.axhline(0, color='red', linestyle='--')
    plt.title('Residuals vs Fitted')
    plt.xlabel('Fitted Values')
    plt.ylabel('Residuals')
    plt.show()

Breusch-Pagan Test (Homoscedasticity)
Purpose: Formal test for heteroscedasticity. It checks if the variance of residuals is constant.
Interpretation: A p-value less than 0.05 indicates heteroscedasticity, suggesting the residuals have non-constant variance.
Code:
    from statsmodels.stats.diagnostic import het_breuschpagan
    bp_test = het_breuschpagan(model.resid, model.model.exog)
    print(f"Breusch-Pagan Test p-value: {bp_test[1]}")

Q-Q Plot (Normality of Residuals)
Purpose: To check if residuals are normally distributed by comparing them to a theoretical normal distribution.
Interpretation: If the points lie approximately along a straight line, the residuals are normally distributed. Deviations indicate non-normality.
Code:
    import statsmodels.api as sm
    sm.qqplot(model.resid, line='45')
    plt.title('Q-Q Plot')
    plt.show()

Shapiro-Wilk Test (Normality of Residuals)
Purpose: A formal statistical test for normality. The null hypothesis is that the residuals follow a normal distribution.
Interpretation: A p-value less than 0.05 indicates that the residuals do not follow a normal distribution.
Code:
    from scipy.stats import shapiro
    stat, p_value = shapiro(model.resid)
    print(f"Shapiro-Wilk Test Stat: {stat}, p-value: {p_value}")

Anderson-Darling Test (Normality of Residuals)
Purpose: Another statistical test for normality of residuals.
Interpretation: A p-value less than 0.05 indicates that the residuals deviate from normality.
Code:
    from scipy.stats import anderson
    result = anderson(model.resid)
    print(f"Anderson-Darling Test Statistic: {result.statistic}, p-value: {result.significance_level}")

Kolmogorov-Smirnov Test (Normality of Residuals)
Purpose: Compares the empirical distribution of residuals with a normal distribution.
Interpretation: A p-value less than 0.05 suggests that the residuals do not follow a normal distribution.
Code:
    from scipy.stats import kstest
    stat, p_value = kstest(model.resid, 'norm')
    print(f"Kolmogorov-Smirnov Test Stat: {stat}, p-value: {p_value}")

 Durbin-Watson Test (Autocorrelation of Residuals)
Purpose: Tests for autocorrelation in residuals, which is especially useful for time series data.
Interpretation: A value close to 2 suggests no autocorrelation. Values close to 0 or 4 indicate positive or negative autocorrelation, respectively.
Code:
    from statsmodels.stats.stattools import durbin_watson
    dw_stat = durbin_watson(model.resid)
    print(f"Durbin-Watson Statistic: {dw_stat}")

Cook's Distance (Influential Points)
Purpose: Identifies influential data points that disproportionately affect the model’s estimates.
Interpretation: Data points with high Cook's distance may be outliers or have a significant influence on the regression model.
Code:
    influence = model.get_influence()
    cooks_d = influence.cooks_distance[0]
    plt.stem(np.arange(len(cooks_d)), cooks_d, markerfmt="o", basefmt=" ", use_line_collection=True)
    plt.title('Cook\'s Distance for Each Data Point')
    plt.xlabel('Index')
    plt.ylabel('Cook\'s Distance')
    plt.show()

Leverage (Influential Points)
Purpose: Measures how much a data point influences the regression line. High leverage points have extreme values for the predictor variables.
Interpretation: Data points with high leverage are more likely to have a large influence on the regression model and should be carefully reviewed.
Code:
    leverage = influence.hat_matrix_diag
    plt.stem(np.arange(len(leverage)), leverage, markerfmt="o", basefmt=" ", use_line_collection=True)
    plt.title('Leverage for Each Data Point')
    plt.xlabel('Index')
    plt.ylabel('Leverage')
    plt.show()

Studentized Residuals (Outliers)
Purpose: Identifies outliers by measuring the difference between the observed value and the predicted value, adjusted for the variance of the residuals.
Interpretation: Large studentized residuals (greater than ±3) indicate outliers.
Code:
    from statsmodels.stats.outliers_influence import OLSInfluence
    influence = OLSInfluence(model)
    studentized_residuals = influence.resid_studentized_internal
    plt.stem(np.arange(len(studentized_residuals)), studentized_residuals, markerfmt="o", basefmt=" ", use_line_collection=True)
    plt.title('Studentized Residuals for Each Data Point')
    plt.xlabel('Index')
    plt.ylabel('Studentized Residuals')
    plt.show()

Breusch-Godfrey Test (Autocorrelation)
Purpose: Detects autocorrelation at higher lags, useful when time-series data has lagged dependencies.
Interpretation: A significant p-value suggests that residuals are autocorrelated at one or more lags.
Code:
    from statsmodels.stats.diagnostic import acorr_breusch_godfrey
    bg_test = acorr_breusch_godfrey(model)
    print(f"Breusch-Godfrey Test p-value: {bg_test[1]}")
